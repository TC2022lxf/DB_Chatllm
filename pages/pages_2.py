import time

import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.outputs import LLMResult
from langchain_core.prompts import ChatPromptTemplate

st.markdown("# test 2 ðŸŽ‰")
st.sidebar.markdown("# Page 2 ðŸŽ‰")
st.sidebar.markdown("## ä¸è¿”å›žç­”æ¡ˆæ¥æº")

from langchain.llms.base import LLM
import pandas as pd
import streamlit as st
import torch
from langchain_community.chat_models import ChatOllama
from langchain_community.llms.ollama import Ollama
from langchain_core.messages import AIMessage, HumanMessage
from utils.retriever import retrievers
from utils.load_data import load_data
from utils.llm import *

class ChainStreamHandler(StreamingStdOutCallbackHandler):
    def __init__(self):
        self.tokens = []
        self.str = ''
        # è®°å¾—ç»“æŸåŽè¿™é‡Œç½®true
        self.finish = False

    def on_llm_new_token(self, token: str, **kwargs):
        print(token)
        self.str +=token
        self.tokens.append(token)

    def on_llm_end(self, response: LLMResult, **kwargs: any) -> None:
        self.finish = 1

    def on_llm_error(self, error: Exception, **kwargs: any) -> None:
        print(str(error))
        self.tokens.append(str(error))

    def generate_tokens(self):
        while not self.finish or self.tokens:
            if self.tokens:
                data = self.tokens.pop(0)
                yield data
            else:
                pass


chainStreamHandler = ChainStreamHandler()
llm = ChatOllama(base_url="http://localhost:11434",
                 model=st.session_state.llm,
                 callback_manager=CallbackManager([chainStreamHandler]))

llm1 = ChatOllama(base_url="http://localhost:11434",
                 model="qwen:14b",
                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))

# åŠ è½½æç¤ºè¯
CONDENSE_QUESTION_PROMPT = PromptTemplate(
    template=st.session_state.CONDENSE_QUESTION_PROMPT, input_variables=["question"]
)
QA_PROMPT = PromptTemplate(
    template=st.session_state.QA_PROMPT, input_variables=["context", "chat_history","question"]
)


# åŠ è½½æ•°æ®åº“
persist_directory = os.path.join('knowledge_base', st.session_state["knowledge_base"])
print(persist_directory)
# embedding model
model = embedding_data()
if os.path.exists(persist_directory) and len(os.listdir(persist_directory)) > 0:  # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©º
    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=model)
else:
    st.error('æ•°æ®åº“é”™è¯¯,å¯èƒ½æ²¡æœ‰åŠ è½½æˆåŠŸ')

if 'chat_history1' not in st.session_state:
    st.session_state['chat_history1'] =[]
if 'messages2' not in st.session_state:  # æ£€æŸ¥ st.session_state ä¸­æ˜¯å¦å­˜åœ¨åä¸º 'messages' çš„é”®
    st.session_state['messages2'] = []
messages = st.session_state.get('messages2', [])  # èŽ·å–messages
print(messages)
for message in messages:  # éåŽ†æ¯ä¸€æ¡ä¿¡æ¯
    if isinstance(message, AIMessage):  # æ£€æŸ¥å˜é‡ message æ˜¯å¦æ˜¯ä¸€ä¸ªåä¸º AIMessage çš„ç±»çš„å®žä¾‹ã€‚
        with st.chat_message('assistant'):  # åˆ›å»ºä¸€ä¸ªèŠå¤©æ¶ˆæ¯æ¡†ï¼Œ
            st.markdown(message.content)  # æ¶ˆæ¯å†…å®¹ä»¥ Markdown æ ¼å¼æ˜¾ç¤ºã€‚
    elif isinstance(message, HumanMessage):
        with st.chat_message('user'):  # åˆ›å»ºä¸€ä¸ªuserçš„èŠå¤©æ¶ˆæ¯æ¡†ï¼Œ
            st.markdown(message.content)

import threading

def async_thread(func, *args, **kwargs):
    thread = threading.Thread(target=func, args=args, kwargs=kwargs)
    thread.start()


if user_input := st.chat_input("Enter your question here"):  # chat_inputåˆ›å»ºèŠå¤©è¾“å…¥æ¡†->user_input
    st.session_state.messages2.append(HumanMessage(content=user_input))  # è¾“å…¥å†…å®¹ä½œä¸ºä¸€ä¸ªHMå¯¹è±¡å®žä¾‹è¢«æ·»åŠ åˆ°st.session_state.messagesåˆ—è¡¨ä¸­
    st.chat_message("user").markdown(user_input)  # æ˜¾ç¤ºç”¨æˆ·åˆšåˆšè¾“å…¥çš„é—®é¢˜ï¼Œæ ‡è®°ä¸ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("assistant"):  # ä¸‹é¢çš„ä»£ç ç”Ÿæˆllmçš„å›žç­”->assistant
        question_generator = LLMChain(llm=llm1, prompt=CONDENSE_QUESTION_PROMPT)
        chain = load_qa_chain(llm, chain_type="stuff", prompt=QA_PROMPT)
        qa = ConversationalRetrievalChain(
            retriever=vectorstore.as_retriever(), combine_docs_chain=chain, question_generator=question_generator,
            return_source_documents=True)
        async_thread(qa,{"question": user_input, "chat_history": st.session_state.chat_history1}, return_only_outputs=True)
        st.write_stream(chainStreamHandler.generate_tokens())
    st.session_state.messages2.append(AIMessage(content=chainStreamHandler.str))
    st.session_state['chat_history1'].append((user_input, chainStreamHandler.str))
