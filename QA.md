# QA-1-transformer框架

## Q-1 总体架构
[alt text](readme _image/image.png)
Transformer模型主要由两个部分组成：编码器（Encoder）和解码器（Decoder）。

 - 编码器（Encoder）：将输入序列编码成一组上下文向量。
 - 解码器（Decoder）：根据编码器生成的上下文向量和解码器自身的输入，生成输出序列。
 - 编码器和解码器分别由多个相同的层（Layer）堆叠而成，通常是6层（可以根据需要调整）。

## Q-2 编码器的输入
### 输入：
#### 1. **输入序列（Input Sequence）**：
   - 一个长度为 $n$ 的词序列 $X = [x _1, x _2, ..., x _n]$，每个$x _i$表示输入序列中的一个词或词片段。




#### 2. **词嵌入（Word Embeddings）**：
   - 输入序列中的每个词$x _i$被转换为一个词嵌入向量 $e _i$，词嵌入矩阵 $E$将所有词映射到一个连续的向量空间中。
   ##### 作用：
词嵌入（Word Embeddings）是将离散的单词表示为连续向量的技术，主要用于将输入序列中的每个词转换为高维向量。词嵌入的主要作用如下：

1. **捕捉词语之间的语义关系**：
   - 词嵌入将语义相似的词映射到相近的向量空间，例如，“king”与“queen”的词嵌入向量距离会比“king”和“dog”的距离更近。通过这种方式，模型可以更好地捕捉词语之间的语义关系。

2. **降维与稠密表示**：
   - 相比于 one-hot 编码方式，词嵌入将词语表示为稠密向量，大大减少了维度（通常是几十到几百维），从而降低了计算复杂度和内存消耗。

3. **增强模型的泛化能力**：
   - 通过预训练的词嵌入（如Word2Vec、GloVe、FastText等），模型可以利用大规模语料库中学到的知识，从而在小数据集上也能表现良好。

##### 举例：
假设输入序列为 "The quick brown fox"，其词嵌入表示为：

```plaintext
"The"  -> [0.1, 0.3, -0.2, ...]
"quick" -> [0.4, -0.1, 0.2, ...]
"brown" -> [0.5, 0.2, 0.3, ...]
"fox"   -> [-0.3, 0.4, 0.1, ...]
```
#### 3. **位置编码（Positional Encoding）**：
   - 由于Transformer没有内置的顺序信息，因此需要显式添加位置编码。位置编码向量 $p _i$与词嵌入向量 $e _i$相加，形成具有位置信息的输入向量：
 $
     h _i^0 = e _i + p _i
    $
   
   整个输入序列被表示为矩阵形式：
 $
     H^0 = [h _1^0, h _2^0, ..., h _n^0] 
    $

##### 作用：
位置编码（Positional Encoding）用于在Transformer模型中显式地引入序列的顺序信息。因为Transformer架构中没有循环神经网络（RNN）或卷积神经网络（CNN）那样的内置位置感知机制，所以需要通过位置编码来提供位置信息。位置编码的主要作用如下：

1. **引入序列位置信息**：
   - 位置编码为每个词嵌入增加唯一的位置信息，使模型能够区分同一词汇在不同位置上的含义。例如，在句子 "He is a good boy" 和 "A good boy is he" 中，“good boy”的位置不同，但词嵌入是相同的。位置编码可以帮助模型识别这种位置信息。

2. **增强模型的表达能力**：
   - 通过将词嵌入与位置编码相加，模型可以同时学习词语的语义信息和位置信息，从而提升模型的表达能力和性能。

3. **保持并行计算的优势**：
   - 位置编码以简单的加法方式引入，不影响模型的并行计算能力。相比于循环神经网络（RNN）逐步处理输入序列，Transformer的并行计算效率更高。

##### 形式：
常用的正弦和余弦函数作为位置编码：

$PE _{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d _{\text{model}}}}\right)$
$PE _{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d _{\text{model}}}}\right)$

其中：
-$pos$是词在序列中的位置。
- $i$是位置编码向量的维度。

### 输出：
- **编码器输出（Encoder Output）**：
  - 编码器输出一个序列的上下文表示 $H^{L} = [h _1^{L}, h _2^{L}, ..., h _n^{L}]$，其中 $L$是编码器层的数量（通常为6）。每个 $h _i^{L}$是包含输入序列上下文信息的向量。
## Q-3: 编码器的框架
每个编码器层包含两个主要子层：

1. **多头自注意力机制（Multi-Head Self-Attention Mechanism）**：
   - 输入序列中的每个词与其他所有词进行交互，生成加权和的表示。
   - 多头机制允许模型从不同的子空间中学习注意力分布。

2. **前馈神经网络（Feed-Forward Neural Network）**：
   - 将自注意力子层的输出进行非线性变换。通常由两层全连接网络组成。

每个子层之后都有一个**残差连接（Residual Connection）**和**层归一化（Layer Normalization）**。

编码器层的结构如下：

```plaintext
                输入向量
                    |
             自注意力机制
                    |
            残差连接和归一化
                    |
             前馈神经网络
                    |
            残差连接和归一化
                    |
                输出向量
```

## Q-4: 解码器的输入输出 
### 输入：
1. **解码器输入序列（Decoder Input Sequence）**：
   - 解码器的输入序列 $Y = [y _1, y _2, ..., y _m]$，每个 $y _i$表示解码器输入序列中的一个词或词片段。通常解码器会在每一步生成一个词，并将其作为下一步的输入。

2. **词嵌入（Word Embeddings）**：
   - 解码器输入序列中的每个词 $y _i$被转换为词嵌入向量 $e _i'$。

3. **位置编码（Positional Encoding）**：
   - 同样地，解码器输入序列中的每个词嵌入向量 $e _i'$与对应的位置信息 $p _i'$相加：
 $
     h _i'^0 = e _i' + p _i'
    $
   
   解码器的输入序列矩阵为：
 $
     H'^0 = [h _1'^0, h _2'^0, ..., h _m'^0]
    $

4. **编码器输出（Encoder Output）**：
   - 来自编码器的上下文表示 $H^{L}$也作为解码器的输入之一。

### 输出：
- **解码器输出（Decoder Output）**：
  - 解码器输出一个序列的预测表示 $H'^{L} = [h _1'^{L}, h _2'^{L}, ..., h _m'^{L}]$，其中 $L$是解码器层的数量（通常为6）。每个 $h _i'^{L}$是解码器生成的表示，结合了编码器输出的上下文信息。


## Q-5: 解码器
每个解码器层包含三个主要子层：

1. **带遮掩的多头自注意力机制（Masked Multi-Head Self-Attention Mechanism）：**
   - 仅关注解码器输入序列中当前词之前的词，防止未来信息泄露。

2. **多头注意力机制（Multi-Head Attention Mechanism）：**
   - 将解码器的当前状态与编码器的输出进行交互，获取上下文信息。

3. **前馈神经网络（Feed-Forward Neural Network）：**
   - 同编码器层。

每个子层之后都有一个**残差连接（Residual Connection）**和**层归一化（Layer Normalization）**。

编码器层的结构如下：

```plaintext
                输入向量
                    |
            遮掩自注意力机制
                    |
            残差连接和归一化
                    |
            多头注意力机制
                    |
            残差连接和归一化
                    |
             前馈神经网络
                    |
            残差连接和归一化
                    |
                输出向量

```

## Q-6: 多头自注意力机制
多头自注意力机制是Transformer的核心创新之一。它将输入向量分别投影到多个子空间，每个子空间计算独立的注意力分数，然后将结果拼接并线性变换。公式如下：

$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d _k}}\right)V$

前置条件：输入向量为$H$,其形状为[n,d<sub>model</sub>],其中n是序列长度，d<sub>model</sub>是输入向量的维度

其中：
- $Q$是查询矩阵（Query）, 其中$Q _i=HW _i^Q$
- $K$是键矩阵（Key）, 其中$K _i=HW _i^K$
- $V$是值矩阵（Value）, 其中$V _i=HW _i^V$
- $d _k$是键向量的维度

对于多头注意力机制，计算会在多个子空间并行进行：

$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head} _1, \text{head} _2, \ldots, \text{head} _h)W^O$

每个头的计算方式为：

$\text{head} _i = \text{Attention}(HW _i^Q, HW _i^K, HW _i^V)$

## Q-7: Transformer的整体输入输出
### 输入：
- **源语言输入序列（Source Language Input Sequence）**：
  - 编码器的输入，即待翻译的文本序列 $X$。
- **目标语言输入序列（Target Language Input Sequence）**：
  - 解码器的输入，即已经生成的目标序列 $Y$（训练过程中是参考翻译，推理过程中是逐步生成的译文）。

### 输出：
- **生成的目标序列（Generated Target Sequence）**：
  - 最终由解码器生成的目标语言文本序列 $Y' = [y _1', y _2', ..., y _m']$，通常通过软最大化（Softmax）函数计算每个时间步上的词概率分布，并选择概率最高的词作为输出。

## Q-8： 示例
假设我们要使用Transformer翻译一个简单的句子 "The quick brown fox jumps over the lazy dog."（源语言）到 "Le renard brun rapide saute par-dessus le chien paresseux."（目标语言）。

### 编码器输入：

- 输入序列：`["The", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"]`
- 词嵌入向量：`[e _1, e _2, ..., e _9]`
- 位置编码向量：`[p _1, p _2, ..., p _9]`
- 输入向量：`[h _1^0, h _2^0, ..., h _9^0]`（词嵌入与位置编码相加）

### 编码器输出：
- 上下文表示：`[h _1^L, h _2^L, ..., h _9^L]`

### 解码器输入：

- 目标序列：`["Le", "renard", "brun", "rapide", "saute", "par-dessus", "le", "chien", "paresseux"]`（训练时使用参考翻译）
- 词嵌入向量：`[e _1', e _2', ..., e _9']`
- 位置编码向量：`[p _1', p _2', ..., p _9']`
- 输入向量：`[h _1'^0, h _2'^0, ..., h _9'^0]`
- 编码器输出：`[h _1^L, h _2^L, ..., h _9^L]`

### 解码器输出：
- 生成的目标序列表示：`[h _1'^L, h _2'^L, ..., h _9'^L]`
- 最终生成的译文：`["Le", "renard", "brun", "rapide", "saute", "par-dessus", "le", "chien", "paresseux"]`

## Q-9：transformer的训练：
- 用于有监督的学习任务，特别是在自然语言处理领域，如机器翻译、文本摘要和问答系统等
### 激活函数：
Transformer模型中使用的激活函数是 ReLU（Rectified Linear Unit）函数，在某些变体中也可能使用 GELU（Gaussian Error Linear Unit）函数。例如，在前馈神经网络（Feed-Forward Neural Network, FFNN）部分，激活函数用于增加非线性：

$$ \text{FFNN}(x) = \max(0, xW _1 + b _1)W _2 + b _2 $$

其中:

- $x$是输入变量
- $W _1$和$W _2$是权重矩阵
- $b _1$和$b _2$是偏置项。

### 损失函数：
损失函数用于评估模型的预测与真实标签之间的差异。在机器翻译等序列生成任务中，通常使用 交叉熵损失（Cross-Entropy Loss）：
$$ \text{Loss} = -\sum _{i=1}^{N} y _i \log(p _i) $$
其中：

- $N$是序列中的词数
- $y _i$是真实标签的独热编码
- $p _i$是模型预测的概率分布
- 损失函数中的$p _i$是与模型参数有关的变量，而$y _i$是与模型参数无关的固定标签。通过调整模型参数，使得$p _i$尽可能接近$y _i $，从而最小化损失函数。


#### 独热编码的生成

独热编码是将分类标签转换为向量的过程。在自然语言处理任务中，每个词都可以被映射到一个唯一的索引，然后转换为一个独热向量。例如，如果词汇表中有10000个词，词 “dog” 的索引是123，那么 “dog” 的独热编码就是一个10000维的向量，其中第123位为1，其余位为0。

### 参数更新
参数更新是通过 反向传播（Backpropagation）和 优化算法（如Adam或SGD）来实现的。Transformer模型特别使用了 学习率调度，其中学习率会随着训练进度进行调整。一个常见的学习率调度策略是 Noam学习率调度，它根据以下公式动态调整学习率：
$$ \text{lr} = d _{\text{model}}^{-0.5} \cdot \min(\text{step\_num}^{-0.5}, \text{step\_num} \cdot \text{warmup\_steps}^{-1.5}) $$

#### 反向传播

反向传播是通过损失函数的梯度来更新模型参数的过程。具体步骤如下：

- 前向传播：通过网络计算输出和损失。
- 计算梯度：计算损失函数相对于每个参数的梯度。
- 更新参数：根据梯度和学习率更新参数。

#### 参数更新基本思想
参数更新是通过优化算法（如Adam或SGD）来实现的。以梯度下降（Gradient Descent）为例，参数更新公式如下：
$$ \theta \leftarrow \theta - \eta \frac{\partial \text{Loss}}{\partial \theta} $$

其中：

- $θ$是模型参数
- $\eta$ 是学习率
- $\frac{\partial \text{Loss}}{\partial \theta}$ 是损失函数相对于参数的梯度。

#### Adam优化算法更新参数
在Transformer中，通常使用Adam优化算法来更新参数，其参数更新公式如下： 

$$ m _t = \beta _1 m _{t-1} + (1 - \beta _1) \nabla _\theta L(\theta _{t-1}) $$
$$ v _t = \beta _2 v _{t-1} + (1 - \beta _2) (\nabla _\theta L(\theta _{t-1}))^2 $$
$$ \hat{m} _t = \frac{m _t}{1 - \beta _1^t} $$
$$ \hat{v} _t = \frac{v _t}{1 - \beta _2^t} $$
$$ \theta _t = \theta _{t-1} - \eta \frac{\hat{m} _t}{\sqrt{\hat{v} _t} + \epsilon} $$

其中：

- $m _t$和$v _t$ 分别是动量项和加速度项的指数加权平均
- $\beta _1$和$\beta _2$是动量项和加速度项的衰减率
- $\eta$ 是学习率。
- $\epsilon$ 是一个小常数，防止除零错误。
- $\nabla _\theta L(\theta _{t-1}) $是损失函数相对于参数的梯度
- $t$为时间步
# QA-2-transformer深度细节

# QA-3-百模预训练框架

# QA-4-经典模型预训练框架-代码细节

# QA-5-模型框架内部多种方法细节-自QA-4衍生
# QA-6-微调方法-框架-细节

# QA-7-微调代码细节

# QA-8-RAG
## RAG pipeline的整个流程
- 请求转换、路由和请求构造、索引和检索、生成和评估
## 为什么要用RAG
1. **新数据**:
   - llms并不了解你的数据，且无法获取与此相关的最新数据，它们是事先使用来自互联网的公共信息训练好的，因此并不是专有数据库的专家也不会针对该数据库进行更新
2. **上下文窗口**
   - 每个llm都有一个tokens的最大限制，用于限制用户每次提交的tokens数据，这会导致丢失上下文窗口之外的上下文，进而影响准确性、产生检索问题和幻觉等
3. **中间遗失**
   - 即使llms可以一次性接收所有数据，但它存在根据信息在文档中的位置来检索信息的问题。[研究表明](https://arxiv.org/abs/2307.03172)如果相关信息位于文档中间(而非开头或结尾)时就会导致严重的性能降级(感知减弱)。

## 请求转换
1. **重写-检索-读取**
   - 这种方式注重对用户的查询进行重构，使用一个llm生成一个查询，然后用检索器来检索内容，llm再回答[rewriter-retriver-reader](https://arxiv.org/abs/2305.14283)。此外这个方法是可训练的，以llm性能为奖励强化学习生成查询的llm。
2. **对问题进行浓缩或者改写**
   - 通常用于会话中，通过把对话改写成一个好的独立问题来给聊天机器人提供上下文
3. **RAG-fusion**
   - 将RAG和倒数排名融合([RRF](https://arxiv.org/abs/2402.03367))结合。生成多个查询(从多个角度添加上下文)，并使用倒数分数($\frac{1}{\text{rank} + k}$)对查询结果重新排序，然后将文档和分数进行融合，从而得到更全面和准确的回答
4. **Step-Back Prompting**
   - 这是一种更加技术性的提示技术，设计stepback-question后通过LLM的抽象来衍生出更高层次的概念和首要原则.根据此概念和首要原则结合问题进行推理。[论文](https://arxiv.org/abs/2310.06117)和[博客](https://blog.csdn.net/m0_49651195/article/details/140265857)
# QA-9-agent
