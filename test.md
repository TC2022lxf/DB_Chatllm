# 自我介绍:
1. 介绍个人信息，姓名、学校、专业、年级，在校获奖如蓝桥杯、robcon、算法比赛、挑战杯等以及每年的奖学金，成绩专业前三，在校项目做过unet医学分割、yolo物体检测、结合llm识别图片等，并申请软著撰写论文，也有成功申请校级省级等项目基金。
24年1月至24年7月在广州晨扬通信公司实习，岗位是大模型算法工程师实习，指导老师是浙大张教授。做的大项目是AI数字人，涉及化妆品、电信、政务等多个方面，应用如微信聊天机器人、电话视频销售、线下店机器人导购、微信抖音微博小红书AI生成文章发布或生成评论引流。我主要是负责数字人的问答，包括但不限于前期的数据收集数据清晰和数据结构设计、lora微调开源大模型qwen2:72b、基于ollama推理框架和langchain框架构建工程代码、设计RAG方案提高llm在未知领域的回答质量、设计agent和tool处理AI数字人的多项功能如订单查询、产品图片、支付链接等等。更加主要的还是对语言模型的回答准确性的负责

# 技术赌博

## 主流的预训练模型
1. 自编码模型：
   - 通过去噪目标学习双向上下文编码器，例如掩码语言模型 (MLM) 。 擅长自然语言理解NLU任务，常用于生成句子的上下文表示，但是不能直接用于文本生成。如情感分类、抽取式问答。代表模型如Bert、RoBERTa、ALBERT
2. 自回归模型：
   - 学习从左到右的语言模型，常用于无条件生成任务(unconditional generation)，如语言建模，具体形如给定一段context，让生成其下文 。代表模型如：GPT、GPT-2、GPT-3
3. encoder-decoder模型：
   - 常用于条件生成任务（conditional generation），比如生成摘要、生成式问答、机器翻译 。代表模型PALM

## 什么是prefix LLM和casual LLM
Prefix LM在Encoder部分采用Auto Encoding (AE-自编码)模式，即前缀序列中任意两个token都相互可见，而Decoder部分采用Auto Regressive(AR-自回归)模式，即待生成的token可以看到Encoder侧所有token(包括上下文)和Decoder侧已经生成的token，但不能看未来尚未产生的token。

Causal LM只涉及到Encoder-Decoder中的Decoder部分，采用Auto Regressive模式，直白地说，就是根据历史的token来预测下一个token，也是在Attention Mask这里做的手脚。



## 介绍以下transformer框架

Transformer模型由编码器和解码器组成。输入词序列通过词嵌入、位置编码后作为编码器的输入，编码器由一个多头注意力层和一个前馈网络层组成，每层都进行了残差连接和归一化，其中注意力层的输入是编码器输入与QKV矩阵相乘，而注意力公式是softmax(QK^T/根号dk)V。关于解码器，对lable词序列进行词嵌入和位置编码后作为解码器的输入，解码器由掩码多头注意力层、多头注意力层、前馈网络层组成，每一层都做了残差连接和归一化,其中多头注意力层的输入是编码器的输出乘KV和上一层的输出与Q相乘。最终解码器的输出经过线性层将输出的高维向量投影的词汇表大小的向量，然后使用softmax将向量转化成概率分布，即每个词出现的概率。

论文中transformer的训练是有监督的。

## Bert模型
BERT 模型的核心思想是通过大规模的无监督预训练来学习通用的语言表示，然后在特定任务上进行微调。BERT 引入了双向 Transformer 编码器，使得模型能够同时利用上下文信息，从而更好地理解词语在不同上下文中的含义。BERT 模型的预训练阶段包含两个任务：Masked Language Model (MLM) 和 Next Sentence Prediction (NSP)。在 MLM 任务中，模型会随机遮盖输入序列的一部分单词，然后预测这些被遮盖的单词。这样的训练方式使得模型能够学习到单词之间的上下文关系。在 NSP 任务中，模型会输入两个句子，并预测这两个句子是否是连续的。这个任务有助于模型理解句子之间的关联性。

## transformer和Bert的区别
Transformer输出：在标准的Transformer模型中，输出是由解码器的最终层产生的，通常是目标序列的预测值，softmax 层生成的每个词的概率分布。
BERT输出：BERT（Bidirectional Encoder Representations from Transformers）是基于Transformer的预训练模型，主要用于学习丰富的上下文语境。BERT的输出不仅包含了整个序列的向量，还包括了每个输入词的上下文相关表示。

总的来说 Transformer 的输出主要用于生成任务，而 BERT 的输出用于表示和理解任务

## 注意力机制都有哪些？能不能简单介绍一下？
- self-attention - 输入序列与QKV参数相乘得到QKV矩阵，根据注意力公式softmax(QKT/根号dk)V 输出结果。
- mutil-head-attention - 由多组attention组成，每组都有对应的QKV参数，最终对每组的输出进行concat得到输出结果
- group-query-attention 组查询注意力机制- 将Q分组，每组对应一个K和V，这样可以减少计算量，而且效果也不差

## 模型的损失函数一般是什么。

## 大模型的幻觉问题、复读机问题是什么，如何解决
幻觉问题：即生成的内容是无意义的或不忠实于提供的源内容。解决方法可以是引入一些外挂知识库、加入一些纠偏规则、限制输出长度等。

复读机问题：重复生成某些话。解决方法可以是丰富数据集的多样性、预处理时尽量过滤重复无意义的文本，同义词替换等做数据增强，温度参数调整，后处理过滤等等方法。


## llama框架
主要还是基于transformer框架，并进行了部分改动，如：

- 对每个子层做输入前置归一化，归一化的函数是RMSNorm，有利于提高训练过程中的稳定性。其中公式是RMS(a)=根号n分之一乘以ai方i=1到n的求和，ai的平均值等于ai除以RMS(a),此外也可以加上缩放因子gi和偏移参数bi
- 用SwiGLU激活函数替换ReLU激活函数。SwiGLU是Swish激活函数的GLU变体，可以用更好的梯度流动性和可能的性能提升
- 从绝对位置嵌入改为旋转嵌入。RoPE通过将位置信息编码为旋转矩阵，使模型能够更加有效地捕捉序列中元素之间的位置关系
- llama2将mutil-head-attention改成group-query-attention
- llama3主要是词表的变化、数据质量的提升
![img.png](img.png)

### LLaMA模型输入句子理论上可以无限长吗？
理论上长度是取决于模型的框架和训练配置，一次性处理长文本的长度是有最大输入长度限制的，如果超出这个这个长度的文本可以采用截断、分段、滑动窗口的方式来处理长文本，但会丢失一些连续的信息。处理时间也比较久。所以理论上是可以无限长的。只是最终的处理策略看是如何设计。
## ChatGLM框架
GLM是一种自回归空白填充的预训练框架，是通用语言模型，可以处理有条件生成任务、无条件生成任务、自然语言理解任务。

glm提出了一个基于新颖的自回归空白填充的目标， 将 NLU 任务构建为包含任务描述的填空题，这些问题通过自回归生成来回答。

比如训练流程是
- (a) 原始文本为 [x1, x2, x3, x4, x5, x6]。两个范围 [x3] 和 [x5, x6] 被采样。

- (b) 用 [M] 替换 Part A 中的采样范围，并在 Part B 中打乱这些范围。

- (c)GLM 自回归地生成 Part B。每个范围前面都加上 [S] 作为输入，后面加上 [E] 作为输出。二维位置编码表示跨范围和范围内的位置。

- (d) 自注意力掩码。灰色区域被屏蔽。Part A 的 tokens 可以关注自己（蓝色框），但不能关注 B。Part B 的 tokens 可以关注 A 和 B 中的前代（黄色和绿色框分别对应两个范围）。

模型结构的不同主要还是
- 注意力掩码矩阵不同、PartA部分内的各token可以互相注意到、PartB部分内的tokens可以注意到PartA和PartB中已经生成的token
- 编码器调整了层归一化和残差连接的顺序、token的预测输出用的是单个的线形层、激活函数从ReLU改成GeLUS
- 位置编码也根据框架特定的任务进行了修改，采用了二维位置编码，即每个标记都用两个位置id编码。

这样的框架可以通过微调适应分类任务和文本生成任务，比较通用。
## qwen框架

## 目前主流的中文向量模型有哪些？
xiaohu-embedding-v2、gte-qwen2-instruct等在huggingface的leaderboard

## 训练
### 什么是Kv cache技术，它具体是如何实现的？
KV cache可以在注意力机制中起到加速作用，在自注意力机制中每个时间步都会计算每个词与其他词的相关性，并生成一组注意力权重，但这样就会出现重复计算某些词的相关性。而缓存机制则计算过的K和V的表示存储在缓存中，这样当有新的输入时，只需要计算新的输入部分的K和V并添加的缓存中，这样就能避免已经计算过的结果重复计算了。
一种常见的实现方式是使用一种称为“缓存表”（Cache Table）的数据结构。将计算的键值对添加到缓存表中，每当有计算新token的时候先查询缓存表找出对应的键值对，如果没有再计算出新的键值对。


## 微调技术
### 参数高效的微调（PEFT）有哪些方法？
- Adapter：在Transformer子层后引入小型全连接网络，训练过程中只更新 Adapter 层的参数。adapter整体的结构包括两个前馈子层和一个非线性层。第一个前馈子层用来降低维度以此减少计算量和参数量，第二个是还原维度并作为adapter的输出，同时整个adapter通过残差连接得到最终输出。
- Prefix-Tuning： 基于提示词前缀优化的微调方法，其原理是在输入token之前构造一段任务相关的virtual tokens（虚拟令牌）作为Prefix，然后训练的时候只更新Prefix部分的参数，而PLM中的其他部分参数固定。其中前缀的初始化方式包括随机初始化、通过预训练的语言模型进行初始化如输出的某一层的隐藏状态、通过特定任务训练出前缀向量。
- P-Tuning - 输入序列构建模板，在序列前面添加提示嵌入。
- P-Tuning v2- 在每一层都加了Prompts tokens作为输入
- Lora - 在某一层旁边加一个参数，原参数冻结，只训练新参数deta W，data W由A和B矩阵组成，分别是d*r和r*d维度，r是秩，并对A高斯初始化，对B初始化为0。为什么低秩的效果那么好呢，因为一个完整的d*d的矩阵是有可能有冗余信息的，可以通过SVD分解成两个矩阵AB，其中AB能近似表示原矩阵，所以我们可以根据这个方法让模型自己学SVD，把秩r当作一个超参，让模型自己学AB矩阵。

### RAG和微调的区别是什么？
- RAG 是从外部文档库中检索相关文档，然后将这些文档与查询一起输入到生成模型中，以生成更加准确和相关的回答。所用到的知识是模型不具备的。
- 微调是通过在特定任务上使用特定知识进一步训练模型来提升性能，此时知识已经是模型具备的了
### 
## 推理
### 简述一下FlashAttention的原理
模型计算的时间复杂度大部分都在attention中，约on方时间复杂度。flash attention的加速原理其实从简单的来说就是通过利用更高速的上层存储计算单元，减少对低速更下层存储器的访问次数，来提升模型的训练性能。

标准的attention对HBM的访问次数为O(Nd+N*2)，当N较大时，HBM的总访问次数会比较昂贵。因此可以尽量利用SRAM进行计算。当SRAM的存储又比较小，所以flashattention就有两个要点：
第一个是平铺：核心思想是原始的注意力矩阵分解成更小的子矩阵，然后分别对这些子矩阵进行计算，只要这个子矩阵的大小可以在SRAM内存放，那么就可以在计算过程中只访问SRAM了。其中提出了分块SoftMax算法，确保了整个Flash Attention的正确性
第二个是重算，这是一种算力换内存的技巧，就是不要存储那么多梯度和每一层的正向传播的中间状态，而是在计算到反向某一层的时候再临时从头开始重算正向传播的中间状态。
### Paged Attention的原理是什么，解决了LLM中的什么问题？
原理一、可在不连续的显存空间存储连续的 key 和 value。用于将每个序列的 KV cache 分块（blocks），每块包含固定数量的 token 的 key 和 value 张量。
原理二、因为 blocks 在显存中不必连续，所以可以更灵活的方式管理键和值：序列的连续逻辑块通过 block table 映射到非连续物理块。 物理块可在生成新 token 时按需分配。因此只有最后一个block会发生显存浪费
原理三：并行采样时，同一个 prompt 生成多个输出序列，这些序列生成时可以共享 prompt 的 attention 计算和显存。
PagedAttention算法在实际应用中可以显著提升大型语言模型的推理速度和吞吐量。


## 什么是 RLHF？
人类反馈强化学习 (RLHF) 是一种机器学习技术，利用人类的直接反馈来训练“奖励模型”，然后利用该模型通过强化学习来优化人工智能坐席的性能。

主要包括三步骤：
预训练一个语言模型 (LM) ；

聚合问答数据并训练一个奖励模型 (Reward Model，RM) ；

用强化学习 (RL) 方式微调 LM。可以选择近端策略优化 (Proximal Policy Optimization，PPO) 微调
## RAG
### RAG技术体系的总体思路
数据预处理-分块-文本向量化-查询向量化-向量检索-检索后处理(如重排、知识压缩)-查询+检索内容输入llm-输出

### 使用外挂知识库主要为了解决什么问题
- 解决模型回答未知领域的问题
- 克服模型淡化某些知识的问题
- 提升回答的准确性、权威性、时效性
- 提高可控性和可解释性，提高模型的可信度和安全性

### 如何评价RAG项目效果
可以使用RAGAS框架来快速进行评测RAG项目的效果，指标可以有：

1. 从用户的query、召回的context、llm的response三方面评估的指标有：
    - context relevance：衡量上下文能够支持query的程度，越高则召回的信息越相关
    - faithfulness： 衡量生成的答案在给定的上下文的一致性
    - answer relevance：衡量生成答案与给定查询提示的相关性。
    - 这些指标如果数据量少的话可以人工评测，数据量多的话可以用强大的llm来评测，因为有个论文llm-as-a-judge通过大量实验得出强大的llm可以匹配人类的偏好达到80.%的一致性，所以用llm也是一种可扩展的评测方式。也可以向量化计算余弦相似度来评分。
    - 如果有标注ground-truth的话，那么就可以直接计算context和answer与标签的相似度进行评分了，但ground-truth人工标注的话虽然真实性高但费时费力，也可以用llm生成， 只要设计好llm的prompt，这也是一种方法。




1. 语义理解（Semantic Understanding）
语义理解是指机器能够理解和解释自然语言中的含义和意图。这涉及多个子任务：

词汇语义分析：识别和理解单词或短语的含义，包括多义词的消歧（Word Sense Disambiguation, WSD）。
句法解析：分析句子的结构，确定各个词语之间的语法关系。
语义角色标注（Semantic Role Labeling, SRL）：确定句子中的各个成分在事件中的角色，如动作、施事、受事等。
共指消解（Coreference Resolution）：识别和链接文本中指代同一实体的不同表达。
命名实体识别（Named Entity Recognition, NER）：识别文本中的专有名词，如人名、地名、组织名等。
语义理解的应用包括对话系统、问答系统、文本分类、信息抽取等。

2. 自动摘要（Automatic Summarization）
自动摘要是指通过计算机自动生成文本的简要版本，同时保留原文的主要信息。自动摘要可以分为两类：

抽取式摘要（Extractive Summarization）：从原文中直接抽取重要的句子或片段组成摘要。这种方法依赖于文本的词频、句子位置等特征来确定重要内容。
生成式摘要（Abstractive Summarization）：通过理解原文内容，用新的表达方式生成简短的摘要。这种方法涉及复杂的自然语言生成技术，需要模型能够理解和重组原文信息。
自动摘要的应用场景包括新闻摘要、文献综述、电子邮件摘要等。

3. 可控文本生成（Controlled Text Generation）
可控文本生成是指根据特定条件或参数生成符合预期的文本。与一般的文本生成不同，可控文本生成可以在以下方面进行控制：

内容主题：生成特定主题或话题的文本。
情感或语气：生成具有特定情感（如积极、消极）或语气（如正式、非正式）的文本。
文本长度：生成具有特定长度的文本。
风格模仿：生成模仿特定作者风格或特定文体的文本。
可控文本生成通常使用条件生成模型（如条件生成对抗网络，Conditional GAN）或预训练语言模型（如GPT-3）来实现。这种技术广泛应用于广告文案生成、对话系统、个性化内容推荐等领域。



## 公式
### 损失函数
![img_1.png](img_1.png)


























多头注意力，频率太高了。coding轮，概念轮都考。复习的点包括：时间/空间复杂度，优化（kv-cache，MQA，GQA），手写多头代码。各种Norm，这个频率也不低，不过比较标准的内容，没有啥特意要说的，有的考手写，有的考概念和理解（为什么管用）。

框架相关内容，各种并行方式，优缺点。DeepSpeed，Megatron可以看看源代码，Flash-Attention等内容。这个点也经常考代码题。

BERT，GPT等比较主流大模型，一些细节，比如位置编码，训练loss，激活，架构些许不同这种。自回归重点。

大模型训练，这个可能主要是工作经验相关，经常问比如训练loss炸掉了，如何解决，一些技巧之类的。面试时有些面试官会问一些很细节的东西，感觉是在确认确实上手跑过基座训练不是吹水。

数据预处理，BPE，tokenization，mask相关概念和对模型/训练影响，数据配比（有paper）。

evaluation，如何评估大模型，安全性，有效性，公开数据，个别考过手写eval框架（多选，生成）。

根据投的岗位，多模态和RLHF内容可以适当看看。这俩感觉paper挺重要的，也大多研究岗位。楼主也少面了一些自动驾驶，RL啥的，不过结果不咋地。