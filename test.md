# 自我介绍:
1. 介绍个人信息，姓名、学校、专业、年级，在校获奖如蓝桥杯、robcon、算法比赛、挑战杯等以及每年的奖学金，成绩专业前三，在校项目做过unet医学分割、yolo物体检测、结合llm识别图片等，并申请软著撰写论文，也有成功申请校级省级等项目基金。
24年1月至24年7月在广州晨扬通信公司实习，岗位是大模型算法工程师实习，指导老师是浙大张教授。做的大项目是AI数字人，涉及化妆品、电信、政务等多个方面，应用如微信聊天机器人、电话视频销售、线下店机器人导购、微信抖音微博小红书AI生成文章发布或生成评论引流。我主要是负责数字人的问答，包括但不限于前期的数据收集数据清晰和数据结构设计、lora微调开源大模型qwen2:72b、基于ollama推理框架和langchain框架构建工程代码、设计RAG方案提高llm在未知领域的回答质量、设计agent和tool处理AI数字人的多项功能如订单查询、产品图片、支付链接等等。更加主要的还是对语言模型的回答准确性的负责

# 技术赌博

## 介绍以下transformer框架

Transformer模型由编码器和解码器组成。输入词序列通过词嵌入、位置编码后作为编码器的输入，编码器由一个多头注意力层和一个前馈网络层组成，每层都进行了残差连接和归一化，其中注意力层的输入是编码器输入与QKV矩阵相乘，而注意力公式是softmax(QK^T/根号dk)V。关于解码器，对lable词序列进行词嵌入和位置编码后作为解码器的输入，解码器由掩码多头注意力层、多头注意力层、前馈网络层组成，每一层都做了残差连接和归一化,其中多头注意力层的输入是编码器的输出乘KV和上一层的输出与Q相乘。最终解码器的输出经过线性层将输出的高维向量投影的词汇表大小的向量，然后使用softmax将向量转化成概率分布，即每个词出现的概率。

论文中transformer的训练是有监督的。

## Bert模型
BERT 模型的核心思想是通过大规模的无监督预训练来学习通用的语言表示，然后在特定任务上进行微调。BERT 引入了双向 Transformer 编码器，使得模型能够同时利用上下文信息，从而更好地理解词语在不同上下文中的含义。BERT 模型的预训练阶段包含两个任务：Masked Language Model (MLM) 和 Next Sentence Prediction (NSP)。在 MLM 任务中，模型会随机遮盖输入序列的一部分单词，然后预测这些被遮盖的单词。这样的训练方式使得模型能够学习到单词之间的上下文关系。在 NSP 任务中，模型会输入两个句子，并预测这两个句子是否是连续的。这个任务有助于模型理解句子之间的关联性。

## transformer和Bert的区别
Transformer输出：在标准的Transformer模型中，输出是由解码器的最终层产生的，通常是目标序列的预测值，softmax 层生成的每个词的概率分布。
BERT输出：BERT（Bidirectional Encoder Representations from Transformers）是基于Transformer的预训练模型，主要用于学习丰富的上下文语境。BERT的输出不仅包含了整个序列的向量，还包括了每个输入词的上下文相关表示。

总的来说 Transformer 的输出主要用于生成任务，而 BERT 的输出用于表示和理解任务

## 注意力机制都有哪些？能不能简单介绍一下？
- self-attention - 输入序列与QKV参数相乘得到QKV矩阵，根据注意力公式softmax(QKT/根号dk)V 输出结果。
- mutil-head-attention - 由多组attention组成，每组都有对应的QKV参数，最终对每组的输出进行concat得到输出结果
- group-query-attention 组查询注意力机制- 将Q分组，每组对应一个K和V，这样可以减少计算量，而且效果也不差

## llama框架

## ChatGLM框架

## qwen框架

## 目前主流的中文向量模型有哪些？
xiaohu-embedding-v2、gte-qwen2-instruct等在huggingface的leaderboard

## 训练
### 什么是Kv cache技术，它具体是如何实现的？
KV cache可以在注意力机制中起到加速作用，在自注意力机制中每个时间步都会计算每个词与其他词的相关性，并生成一组注意力权重，但这样就会出现重复计算某些词的相关性。而缓存机制则计算过的K和V的表示存储在缓存中，这样当有新的输入时，只需要计算新的输入部分的K和V并添加的缓存中，这样就能避免已经计算过的结果重复计算了。
一种常见的实现方式是使用一种称为“缓存表”（Cache Table）的数据结构。将计算的键值对添加到缓存表中，每当有计算新token的时候先查询缓存表找出对应的键值对，如果没有再计算出新的键值对。


## 微调技术
### 参数高效的微调（PEFT）有哪些方法？
- Adapter：在Transformer子层后引入小型全连接网络，训练过程中只更新 Adapter 层的参数。，这种方法被广泛采用。adapter整体的结构包括两个前馈子层和一个非线性层。第一个前馈子层用来降低维度以此减少计算量和参数量，第二个是还原维度并作为adapter的输出，同时整个adapter通过残差连接得到最终输出。
- Prefix-Tuning： 基于提示词前缀优化的微调方法，其原理是在输入token之前构造一段任务相关的virtual tokens（虚拟令牌）作为Prefix，然后训练的时候只更新Prefix部分的参数，而PLM中的其他部分参数固定。其中前缀的初始化方式包括随机初始化、通过预训练的语言模型进行初始化如输出的某一层的隐藏状态、通过特定任务训练出前缀向量。
- P-Tuning - 输入序列构建模板，在序列前面添加提示嵌入。
- P-Tuning - 在每一层都加了Prompts tokens作为输入
- Lora - 在某一层旁边加一个参数，原参数冻结，只训练新参数deta W，data W由A和B矩阵组成，分别是d*r和r*d维度，r是秩，并对A高斯初始化，对B初始化为0。为什么低秩的效果那么好呢，因为一个完整的d*d的矩阵是有可能有冗余信息的，可以通过SVD分解成两个矩阵AB，其中AB能近似表示原矩阵，所以我们可以根据这个方法让模型自己学SVD，把秩r当作一个超参，让模型自己学AB矩阵。

### RAG和微调的区别是什么？
- RAG 是从外部文档库中检索相关文档，然后将这些文档与查询一起输入到生成模型中，以生成更加准确和相关的回答。所用到的知识是模型不具备的。
- 微调是通过在特定任务上使用特定知识进一步训练模型来提升性能，此时知识已经是模型具备的了
### 
## 推理
### 简述一下FlashAttention的原理
### Paged Attention的原理是什么，解决了LLM中的什么问题？


多头注意力，频率太高了。coding轮，概念轮都考。复习的点包括：时间/空间复杂度，优化（kv-cache，MQA，GQA），手写多头代码。各种Norm，这个频率也不低，不过比较标准的内容，没有啥特意要说的，有的考手写，有的考概念和理解（为什么管用）。

框架相关内容，各种并行方式，优缺点。DeepSpeed，Megatron可以看看源代码，Flash-Attention等内容。这个点也经常考代码题。

BERT，GPT等比较主流大模型，一些细节，比如位置编码，训练loss，激活，架构些许不同这种。自回归重点。

大模型训练，这个可能主要是工作经验相关，经常问比如训练loss炸掉了，如何解决，一些技巧之类的。面试时有些面试官会问一些很细节的东西，感觉是在确认确实上手跑过基座训练不是吹水。

数据预处理，BPE，tokenization，mask相关概念和对模型/训练影响，数据配比（有paper）。

evaluation，如何评估大模型，安全性，有效性，公开数据，个别考过手写eval框架（多选，生成）。

根据投的岗位，多模态和RLHF内容可以适当看看。这俩感觉paper挺重要的，也大多研究岗位。楼主也少面了一些自动驾驶，RL啥的，不过结果不咋地。